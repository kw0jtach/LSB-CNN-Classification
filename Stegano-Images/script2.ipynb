{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc1c0f9",
   "metadata": {},
   "source": [
    "## Change some Images from RGBA to RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e98aee5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RGB images: 32000\n",
      "Images with non-RGB modes or errors: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Paths to your datasets\n",
    "train_dir = 'stegoimagesdataset/train/train'\n",
    "val_dir = 'stegoimagesdataset/val/val'\n",
    "test_dir = 'stegoimagesdataset/test/test'\n",
    "\n",
    "def find_non_rgb_images(directories):\n",
    "    non_rgb = []\n",
    "    rgb_count = 0\n",
    "\n",
    "    for root in directories:\n",
    "        for dirpath, _, filenames in os.walk(root):\n",
    "            for fname in filenames:\n",
    "                if fname.lower().endswith('.png'):\n",
    "                    path = os.path.join(dirpath, fname)\n",
    "                    try:\n",
    "                        with Image.open(path) as img:\n",
    "                            if img.mode != 'RGB':\n",
    "                                non_rgb.append((path, img.mode))\n",
    "                            else:\n",
    "                                rgb_count += 1\n",
    "                    except Exception as e:\n",
    "                        non_rgb.append((path, f'Error opening: {e}'))\n",
    "\n",
    "    return rgb_count, non_rgb\n",
    "\n",
    "# Run the check\n",
    "dirs = [train_dir, val_dir, test_dir]\n",
    "rgb_count, non_rgb_images = find_non_rgb_images(dirs)\n",
    "\n",
    "print(f\"Total RGB images: {rgb_count}\")\n",
    "print(f\"Images with non-RGB modes or errors: {len(non_rgb_images)}\")\n",
    "for path, mode in non_rgb_images:\n",
    "    print(f\"  {mode}: {path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9828af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def convert_clean_rgba_to_rgb(root_dirs):\n",
    "    \"\"\"\n",
    "    Parcourt uniquement les sous-dossiers 'clean' de chacun des répertoires racines\n",
    "    et convertit les images PNG RGBA en RGB.\n",
    "    \"\"\"\n",
    "    for root in root_dirs:\n",
    "        clean_dir = os.path.join(root, 'clean')\n",
    "        if not os.path.isdir(clean_dir):\n",
    "            continue\n",
    "\n",
    "        for fname in os.listdir(clean_dir):\n",
    "            if not fname.lower().endswith('.png'):\n",
    "                continue\n",
    "\n",
    "            path = os.path.join(clean_dir, fname)\n",
    "            try:\n",
    "                with Image.open(path) as img:\n",
    "                    if img.mode == 'RGBA':\n",
    "                        rgb_img = img.convert('RGB')\n",
    "                        rgb_img.save(path)\n",
    "                        print(f\"Converted RGBA → RGB: {path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {path}: {e}\")\n",
    "\n",
    "# Exemple d'utilisation :\n",
    "train_dir = 'stegoimagesdataset/train/train'\n",
    "val_dir   = 'stegoimagesdataset/val/val'\n",
    "test_dir  = 'stegoimagesdataset/test/test'\n",
    "\n",
    "convert_clean_rgba_to_rgb([train_dir, val_dir, test_dir])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c64648",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3c0635",
   "metadata": {},
   "source": [
    "### Customed Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6097546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class CustomCNNFusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # [B, 32, 224, 224]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # [B, 32, 112, 112]\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # [B, 64, 56, 56]\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # [B, 128, 28, 28]\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * 28 * 28 + 8, 128),  # +8 pour les features statistiques\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)  # 2 classes : normal / steg\n",
    "        )\n",
    "\n",
    "    def forward(self, x_img, x_stats):\n",
    "        x = self.cnn(x_img)\n",
    "        x = self.flatten(x)              # [B, 128 * 28 * 28]\n",
    "        x = torch.cat((x, x_stats), dim=1)  # fusion avec features stats\n",
    "        return self.fc(x)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class ResStatFusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        base_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.cnn = nn.Sequential(*list(base_model.children())[:-1])  # form of the tensor just after the last layer :\n",
    "        #[B, 512, 1, 1], Remove the last fully connected layer and keep the pipeline which extracts features\n",
    "        # from the image. The last layer is a fully connected layer that outputs 1000 classes (ImageNet).\n",
    "        \"\"\"\n",
    "        Why 512 ? : \n",
    "        ResNet18 has 512 channels(features maps, filters) in the last convolutional layer before the fully connected layer.\n",
    "        Each convolutional filter in the last layer yields a feature map, and the number of filters in the last layer is 512.\n",
    "        The output of the last convolutional layer is a tensor with shape [B, 512, 1, 1], where B is the batch size.\n",
    "        The 1x1 spatial dimension indicates that the feature maps have been pooled down to a single value per channel.\n",
    "        The AvgPool2d layer reduces(computes the average of the pixels in the image) the spatial dimensions to 1x1, effectively summarizing each feature map into a single value.\n",
    "        So in the end, we get a tensor of shape [B, 512] after the squeeze operation.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self.stat_fc = nn.Sequential( #pipeline for the statistical features\n",
    "            nn.Linear(8, 64), # [B, 8] -> [B, 64] : output = input * weight + bias\n",
    "            nn.ReLU(), # activation function : f(x) = max(0, x)\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # fusion layer\n",
    "        self.final_fc = nn.Sequential(\n",
    "            nn.Linear(512 + 64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, stat_feats):\n",
    "        # CNN branch\n",
    "        cnn_feat = self.cnn(image).squeeze()  # [B, 512], we dont need 1x1 spatial dimension, so we remove it with squeeze()\n",
    "        if cnn_feat.dim() == 1: # If the batch size is 1, add a dimension to make it [1, 512]\n",
    "            cnn_feat = cnn_feat.unsqueeze(0)\n",
    "\n",
    "        # MLP branch\n",
    "        stat_feat = self.stat_fc(stat_feats)  # [B, 64], passing the statistical features through the MLP\n",
    "\n",
    "        fusion = torch.cat((cnn_feat, stat_feat), dim=1)  # [B, 576], concatenate the two branches\n",
    "        out = self.final_fc(fusion) # [B, 2], passing through the final fully connected layer\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8845c1",
   "metadata": {},
   "source": [
    "### Initialization of the sets and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e9b62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "  Image shape: torch.Size([64, 3, 224, 224])\n",
      "  Stat features shape: torch.Size([64, 8])\n",
      "  Label shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from custom_dataset import FusionFeatureDataset\n",
    "torch.backends.cudnn.benchmark = True \n",
    "\n",
    "class RandomRotation:\n",
    "    def __call__(self, img):\n",
    "        angles = [0, 90, 180, 270]\n",
    "        angle = random.choice(angles)\n",
    "        return TF.rotate(img, angle)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    #RandomRotation(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dir = 'stegoimagesdataset/train/train/'\n",
    "val_dir = 'stegoimagesdataset/val/val/'\n",
    "test_dir = 'stegoimagesdataset/test/test/'\n",
    "\n",
    "train_dataset = FusionFeatureDataset(root_dir=train_dir, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True, num_workers=4, persistent_workers=True, prefetch_factor=2)\n",
    "\n",
    "val_dataset = FusionFeatureDataset(root_dir=val_dir, transform=transform)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "test_dataset = FusionFeatureDataset(root_dir=test_dir, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "for i, (img, stat_feats, label) in enumerate(train_loader):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(f\"  Image shape: {img.shape}\")\n",
    "    print(f\"  Stat features shape: {stat_feats.shape}\")\n",
    "    print(f\"  Label shape: {label.shape}\")\n",
    "    break  # Just to show the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bb1e7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51f8486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "class_counts = [4000, 12000]\n",
    "total = sum(class_counts)\n",
    "class_weights = [total / c for c in class_counts]  # inverse fréquence\n",
    "\n",
    "weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, device, patience, save_path):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    scaler = GradScaler() \n",
    "    print(f\"Starting training for model: {model.__class__.__name__} at {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())} for {num_epochs} epochs\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, stat_feats, labels in train_loader:\n",
    "            images, stat_feats, labels = images.to(device, non_blocking=True), stat_feats.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(images, stat_feats)\n",
    "                loss = criterion(outputs, labels)\n",
    "            #loss.backward()\n",
    "            #optimizer.step()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            print(total)\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = correct / total * 100\n",
    "        print(f\"[Train] Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.2f}%\")\n",
    "\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for val_images, val_feats, val_labels in val_loader:\n",
    "                val_images, val_feats, val_labels = val_images.to(device), val_feats.to(device), val_labels.to(device)\n",
    "                val_outputs = model(val_images, val_feats)\n",
    "                _, val_pred = torch.max(val_outputs, 1)\n",
    "                val_correct += (val_pred == val_labels).sum().item()\n",
    "                val_total += val_labels.size(0)\n",
    "\n",
    "        val_acc = val_correct / val_total * 100\n",
    "        print(f\"[Validation] Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "        # === Early Stopping & Best Model Saving ===\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(\"Best model saved.\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"No improvement. ({epochs_without_improvement}/{patience})\")\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    print(f\"Training finished. Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad91033a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for model: CustomCNNFusion at 2025-04-25 17:03:17 for 30 epochs\n",
      "64\n",
      "128\n",
      "192\n",
      "256\n",
      "320\n",
      "384\n",
      "448\n",
      "512\n",
      "576\n",
      "640\n",
      "704\n",
      "768\n",
      "832\n",
      "896\n",
      "960\n",
      "1024\n",
      "1088\n",
      "1152\n",
      "1216\n",
      "1280\n",
      "1344\n",
      "1408\n",
      "1472\n",
      "1536\n",
      "1600\n",
      "1664\n",
      "1728\n",
      "1792\n",
      "1856\n",
      "1920\n",
      "1984\n",
      "2048\n",
      "2112\n",
      "2176\n",
      "2240\n",
      "2304\n",
      "2368\n",
      "2432\n",
      "2496\n",
      "2560\n",
      "2624\n",
      "2688\n",
      "2752\n",
      "2816\n",
      "2880\n",
      "2944\n",
      "3008\n",
      "3072\n",
      "3136\n",
      "3200\n",
      "3264\n",
      "3328\n",
      "3392\n",
      "3456\n",
      "3520\n",
      "3584\n",
      "3648\n",
      "3712\n",
      "3776\n",
      "3840\n",
      "3904\n",
      "3968\n",
      "4032\n",
      "4096\n",
      "4160\n",
      "4224\n",
      "4288\n",
      "4352\n",
      "4416\n",
      "4480\n",
      "4544\n",
      "4608\n",
      "4672\n",
      "4736\n",
      "4800\n",
      "4864\n",
      "4928\n",
      "4992\n",
      "5056\n",
      "5120\n",
      "5184\n",
      "5248\n",
      "5312\n",
      "5376\n",
      "5440\n",
      "5504\n",
      "5568\n",
      "5632\n",
      "5696\n",
      "5760\n",
      "5824\n",
      "5888\n",
      "5952\n",
      "6016\n",
      "6080\n",
      "6144\n",
      "6208\n",
      "6272\n",
      "6336\n",
      "6400\n",
      "6464\n",
      "6528\n",
      "6592\n",
      "6656\n",
      "6720\n",
      "6784\n",
      "6848\n",
      "6912\n",
      "6976\n",
      "7040\n",
      "7104\n",
      "7168\n",
      "7232\n",
      "7296\n",
      "7360\n",
      "7424\n",
      "7488\n",
      "7552\n",
      "7616\n",
      "7680\n",
      "7744\n",
      "7808\n",
      "7872\n",
      "7936\n",
      "8000\n",
      "8064\n",
      "8128\n",
      "8192\n",
      "8256\n",
      "8320\n",
      "8384\n",
      "8448\n",
      "8512\n",
      "8576\n",
      "8640\n",
      "8704\n",
      "8768\n",
      "8832\n",
      "8896\n",
      "8960\n",
      "9024\n",
      "9088\n",
      "9152\n",
      "9216\n",
      "9280\n",
      "9344\n",
      "9408\n",
      "9472\n",
      "9536\n",
      "9600\n",
      "9664\n",
      "9728\n",
      "9792\n",
      "9856\n",
      "9920\n",
      "9984\n",
      "10048\n",
      "10112\n",
      "10176\n",
      "10240\n",
      "10304\n",
      "10368\n",
      "10432\n",
      "10496\n",
      "10560\n",
      "10624\n",
      "10688\n",
      "10752\n",
      "10816\n",
      "10880\n",
      "10944\n",
      "11008\n",
      "11072\n",
      "11136\n",
      "11200\n",
      "11264\n",
      "11328\n",
      "11392\n",
      "11456\n",
      "11520\n",
      "11584\n",
      "11648\n",
      "11712\n",
      "11776\n",
      "11840\n",
      "11904\n",
      "11968\n",
      "12032\n",
      "12096\n",
      "12160\n",
      "12224\n",
      "12288\n",
      "12352\n",
      "12416\n",
      "12480\n",
      "12544\n",
      "12608\n",
      "12672\n",
      "12736\n",
      "12800\n",
      "12864\n",
      "12928\n",
      "12992\n",
      "13056\n",
      "13120\n",
      "13184\n",
      "13248\n",
      "13312\n",
      "13376\n",
      "13440\n",
      "13504\n",
      "13568\n",
      "13632\n",
      "13696\n",
      "13760\n",
      "13824\n",
      "13888\n",
      "13952\n",
      "14016\n",
      "14080\n",
      "14144\n",
      "14208\n",
      "14272\n",
      "14336\n",
      "14400\n",
      "14464\n",
      "14528\n",
      "14592\n",
      "14656\n",
      "14720\n",
      "14784\n",
      "14848\n",
      "14912\n",
      "14976\n",
      "15040\n",
      "15104\n",
      "15168\n",
      "15232\n",
      "15296\n",
      "15360\n",
      "15424\n",
      "15488\n",
      "15552\n",
      "15616\n",
      "15680\n",
      "15744\n",
      "15808\n",
      "15872\n",
      "15936\n",
      "16000\n",
      "[Train] Epoch 1/30 | Loss: 0.7055 | Accuracy: 52.89%\n",
      "[Validation] Accuracy: 25.07%\n",
      "Best model saved.\n",
      "64\n",
      "128\n",
      "192\n",
      "256\n",
      "320\n",
      "384\n",
      "448\n",
      "512\n",
      "576\n",
      "640\n",
      "704\n",
      "768\n",
      "832\n",
      "896\n",
      "960\n",
      "1024\n",
      "1088\n",
      "1152\n",
      "1216\n",
      "1280\n",
      "1344\n",
      "1408\n",
      "1472\n",
      "1536\n",
      "1600\n",
      "1664\n",
      "1728\n",
      "1792\n",
      "1856\n",
      "1920\n",
      "1984\n",
      "2048\n",
      "2112\n",
      "2176\n",
      "2240\n",
      "2304\n",
      "2368\n",
      "2432\n",
      "2496\n",
      "2560\n",
      "2624\n",
      "2688\n",
      "2752\n",
      "2816\n",
      "2880\n",
      "2944\n",
      "3008\n",
      "3072\n",
      "3136\n",
      "3200\n",
      "3264\n",
      "3328\n",
      "3392\n",
      "3456\n",
      "3520\n",
      "3584\n",
      "3648\n",
      "3712\n",
      "3776\n",
      "3840\n",
      "3904\n",
      "3968\n",
      "4032\n",
      "4096\n",
      "4160\n",
      "4224\n",
      "4288\n",
      "4352\n",
      "4416\n",
      "4480\n",
      "4544\n",
      "4608\n",
      "4672\n",
      "4736\n",
      "4800\n",
      "4864\n",
      "4928\n",
      "4992\n",
      "5056\n",
      "5120\n",
      "5184\n",
      "5248\n",
      "5312\n",
      "5376\n",
      "5440\n",
      "5504\n",
      "5568\n",
      "5632\n",
      "5696\n",
      "5760\n",
      "5824\n",
      "5888\n",
      "5952\n",
      "6016\n",
      "6080\n",
      "6144\n",
      "6208\n",
      "6272\n",
      "6336\n",
      "6400\n",
      "6464\n",
      "6528\n",
      "6592\n",
      "6656\n",
      "6720\n",
      "6784\n",
      "6848\n",
      "6912\n",
      "6976\n",
      "7040\n",
      "7104\n",
      "7168\n",
      "7232\n",
      "7296\n",
      "7360\n",
      "7424\n",
      "7488\n",
      "7552\n",
      "7616\n",
      "7680\n",
      "7744\n",
      "7808\n",
      "7872\n",
      "7936\n",
      "8000\n",
      "8064\n",
      "8128\n",
      "8192\n",
      "8256\n",
      "8320\n",
      "8384\n",
      "8448\n",
      "8512\n",
      "8576\n",
      "8640\n",
      "8704\n",
      "8768\n",
      "8832\n",
      "8896\n",
      "8960\n",
      "9024\n",
      "9088\n",
      "9152\n",
      "9216\n",
      "9280\n",
      "9344\n",
      "9408\n",
      "9472\n",
      "9536\n",
      "9600\n",
      "9664\n",
      "9728\n",
      "9792\n",
      "9856\n",
      "9920\n",
      "9984\n",
      "10048\n",
      "10112\n",
      "10176\n",
      "10240\n",
      "10304\n",
      "10368\n",
      "10432\n",
      "10496\n",
      "10560\n",
      "10624\n",
      "10688\n",
      "10752\n",
      "10816\n",
      "10880\n",
      "10944\n",
      "11008\n",
      "11072\n",
      "11136\n",
      "11200\n",
      "11264\n",
      "11328\n",
      "11392\n",
      "11456\n",
      "11520\n",
      "11584\n",
      "11648\n",
      "11712\n",
      "11776\n",
      "11840\n",
      "11904\n",
      "11968\n",
      "12032\n",
      "12096\n",
      "12160\n",
      "12224\n",
      "12288\n",
      "12352\n",
      "12416\n",
      "12480\n",
      "12544\n",
      "12608\n",
      "12672\n",
      "12736\n",
      "12800\n",
      "12864\n",
      "12928\n",
      "12992\n",
      "13056\n",
      "13120\n",
      "13184\n",
      "13248\n",
      "13312\n",
      "13376\n",
      "13440\n",
      "13504\n",
      "13568\n",
      "13632\n",
      "13696\n",
      "13760\n",
      "13824\n",
      "13888\n",
      "13952\n",
      "14016\n",
      "14080\n",
      "14144\n",
      "14208\n",
      "14272\n",
      "14336\n",
      "14400\n",
      "14464\n",
      "14528\n",
      "14592\n",
      "14656\n",
      "14720\n",
      "14784\n",
      "14848\n",
      "14912\n",
      "14976\n",
      "15040\n",
      "15104\n",
      "15168\n",
      "15232\n",
      "15296\n",
      "15360\n",
      "15424\n",
      "15488\n",
      "15552\n",
      "15616\n",
      "15680\n",
      "15744\n",
      "15808\n",
      "15872\n",
      "15936\n",
      "16000\n",
      "[Train] Epoch 2/30 | Loss: 0.6941 | Accuracy: 52.04%\n",
      "[Validation] Accuracy: 46.92%\n",
      "Best model saved.\n",
      "64\n",
      "128\n",
      "192\n",
      "256\n",
      "320\n",
      "384\n",
      "448\n",
      "512\n",
      "576\n",
      "640\n",
      "704\n",
      "768\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m model = CustomCNNFusion()\n\u001b[32m      2\u001b[39m model2 = ResStatFusion()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m trained_model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbest_model_CNNFUSION.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m trained_model2 = train_model(model2, train_loader, val_loader, num_epochs=\u001b[32m30\u001b[39m, device=device, patience=\u001b[32m5\u001b[39m, save_path=\u001b[33m'\u001b[39m\u001b[33mbest_model_ResStatFusion.pth\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, num_epochs, device, patience, save_path)\u001b[39m\n\u001b[32m     23\u001b[39m correct = \u001b[32m0\u001b[39m\n\u001b[32m     24\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstat_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstat_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstat_feats\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1447\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1451\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1402\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1400\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1401\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1403\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1404\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1230\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1231\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1232\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1240\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1241\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1242\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1243\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1244\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1245\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1246\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1247\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1248\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\torch_cuda\\Lib\\queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\torch_cuda\\Lib\\threading.py:331\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    333\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = CustomCNNFusion()\n",
    "model2 = ResStatFusion()\n",
    "trained_model = train_model(model, train_loader, val_loader, num_epochs=30, device=device, patience=5, save_path='best_model_CNNFUSION.pth')\n",
    "trained_model2 = train_model(model2, train_loader, val_loader, num_epochs=30, device=device, patience=5, save_path='best_model_ResStatFusion.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b0528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = FusionFeatureDataset(root_dir=test_dir, transform=transform, device=device)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4adb617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_model(model_class, test_loader, model_path, device):\n",
    "    model = model_class().to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval() # set to evaluation mode, deactivate dropout and batch normalization\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad(): # deactivate gradient calculation for inference\n",
    "        for images, stat_feats, labels in test_loader:\n",
    "            images, stat_feats, labels = images.to(device), stat_feats.to(device), labels.to(device)\n",
    "            outputs = model(images, stat_feats)\n",
    "            _, predicted = torch.max(outputs, 1) # returns the max values and the indices, select the index of the max value\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    labels_names = ['Normal (0)', 'Stego (1)']\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels_names, yticklabels=labels_names)\n",
    "    plt.xlabel('Prédit')\n",
    "    plt.ylabel('Vrai')\n",
    "    plt.title('Matrice de confusion')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Rapport de classification :\")\n",
    "    print(classification_report(y_true, y_pred, target_names=labels_names))\n",
    "\n",
    "test_model(CustomCNNFusion, test_loader, 'best_model_CNNFUSION.pth', device)\n",
    "test_model(ResStatFusion, test_loader, 'best_model_ResStatFusion.pth', device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
